# -*- coding: utf-8 -*-
"""v1.1_credit_card_fraud_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dkrqGeMQEVcZS8Q3EqycOnvA39-kmFuS

# Importing
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sb
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
import warnings
from IPython.display import clear_output
!pip install pydot
import pydot
from IPython.display import Image
from six import StringIO
from sklearn.tree import export_graphviz
from sklearn.preprocessing import StandardScaler, MaxAbsScaler, Binarizer, OneHotEncoder, FunctionTransformer
from sklearn.datasets import load_digits
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.pipeline import make_pipeline

import time

# modeling:
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score

# XGboost
import xgboost as xgb
from xgboost import XGBClassifier, plot_tree

# over sampling and under sampling
!pip install -U imbalanced-learn
from imblearn.over_sampling import RandomOverSampler, SMOTE
from imblearn.under_sampling import RandomUnderSampler, NearMiss
from collections import Counter
from pandas.core.common import random_state

"""#Load Data - abstract data set for Credit card fraud detection

Use this data set to develop Models to detect credit card fraud.

https://www.kaggle.com/datasets/shubhamjoshi2130of/abstract-data-set-for-credit-card-fraud-detection
"""

def csv_drive_path_generatoer(url):
 path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
 return path


card_transdata_url = 'https://drive.google.com/file/d/1FT1xEhnSDZWSB8ceHu0MEQZrFNCb3o1O/view?usp=sharing'
data_url = csv_drive_path_generatoer(card_transdata_url)

df = pd.read_csv(data_url, header=0, squeeze=True, error_bad_lines=False)

"""# EDA

##Data overview
"""

df

# renaming columns

df.rename(columns={'Merchant_id': 'merchant_id',
                   'Transaction date': 'trans_date',
                   'Average Amount/transaction/day': 'avg_amount_trans_per_day',
                   'Transaction_amount': 'trans_amount',
                   'Is declined': 'is_decline',
                   'Total Number of declines/day': 'total_decline_per_day',
                   'isForeignTransaction': 'is_foreign_trans',
                   'isHighRiskCountry': 'is_high_risk_country',
                   'Daily_chargeback_avg_amt': 'avg_daily_chargeback_amount',
                   '6_month_avg_chbk_amt': '6_month_avg_chargeback_amount',
                   '6-month_chbk_freq': '6_month_chbk_freq',
                   'isFradulent': 'is_fraud'}, inplace=True)

df

print('\n INFO: \n')
print(df.info())

print('\n DESCRIBE: \n')
df.describe()

print('\n ISNULL: \n')
print(df.isnull().sum())

print('\n NUNIQUE \n')
print(df.nunique())

"""##Features proceccing"""

# dropping columns of irelevant data
df = df.drop('merchant_id', axis=1)
df = df.drop('trans_date', axis=1)

# Converint Y/N features to 
df=pd.get_dummies(df, columns=['is_decline','is_foreign_trans','is_high_risk_country', 'is_fraud'],drop_first=True)
df

"""## presenting Froud/non_Fraud ditribution"""

# target variable
fraud_value_counts = df.is_fraud_Y.value_counts()
num = df.is_fraud_Y.value_counts(normalize=True)
fmt = '{:.1%}'.format
percent_map_format =  pd.DataFrame({'num': num, 'percent': num.map(fmt)})

percent_map_format_new = percent_map_format.drop('num', axis=1)

# print(f"\nis_fraud_Y count (%):\n\n{percent_map_format_new}") 
df_fraud_counts =  pd.DataFrame({'fraud count': fraud_value_counts, 'fraud percent': num.map(fmt)})
df_fraud_counts_num =  pd.DataFrame({'fraud count': fraud_value_counts, 'fraud percent': num})

df_fraud_counts

# Make the plot with pandas
df_fraud_counts_num['fraud percent'].plot(kind='pie', autopct='%.1f', labels=df_fraud_counts_num.index, subplots=True, figsize=(5, 5))
plt.title("fraud counts dist")
plt.ylabel("")
plt.show()

df_hist = df

df_hist = df_hist.drop('is_decline_Y', axis=1)
df_hist = df_hist.drop('is_foreign_trans_Y', axis=1)
df_hist = df_hist.drop('is_high_risk_country_Y', axis=1)
df_hist = df_hist.drop('is_fraud_Y', axis=1)


df_hist.hist(bins = 15, figsize=(15,15), color = "C0")
plt.show();

df_hist = df

df_hist = df_hist.drop('is_decline_Y', axis=1)
df_hist = df_hist.drop('is_foreign_trans_Y', axis=1)
df_hist = df_hist.drop('is_high_risk_country_Y', axis=1)
# df_hist = df_hist.drop('is_fraud_Y', axis=1)

# df_hist.hist(bins = 15, figsize=(15,15), color = "C0")
# plt.show();

sb.pairplot(df_hist, hue="is_fraud_Y", height=3)

df_hist = df
df_hist = df_hist.drop('avg_amount_trans_per_day', axis=1)
df_hist = df_hist.drop('avg_daily_chargeback_amount', axis=1)
df_hist = df_hist.drop('6_month_avg_chargeback_amount', axis=1)
df_hist = df_hist.drop('6_month_chbk_freq', axis=1)


df_hist = df_hist.drop('is_decline_Y', axis=1)
df_hist = df_hist.drop('is_foreign_trans_Y', axis=1)
df_hist = df_hist.drop('is_high_risk_country_Y', axis=1)
# df_hist = df_hist.drop('is_fraud_Y', axis=1)

# df_hist.hist(bins = 15, figsize=(15,15), color = "C0")
# plt.show();

sb.pairplot(df_hist, hue="is_fraud_Y", height=3)

"""#Modeling

## Preparing the data for models

### Separate X and y data sets
"""

# features
X = df.drop('is_fraud_Y', axis=1)

# target variable
y = df.is_fraud_Y

"""###Split to Train , Test and Validation sets"""

# split Train test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)

# resplit the test set to valid set
X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, random_state=5678) # was changed by tuval to 50%

print('X_train shape')
print(X_train.shape)
print('X_test shape')
print(X_test.shape)
print('X_valid shape')
print(X_valid.shape)

# decide the performance metric:
# recall
# precision

# scalar for columns:
# Average Amount
# Transaction_amount
# Total Number of declines
# Daily_chargeback_avg_amt
# 6_month_avg_chbk_amt
# 6-month_chbk_freq  


# fraud 
# Is declined_Y
# isForeignTransaction_Y  
# isHighRiskCountry_Y



# Data Enrichment:

# Data Transformations:

# TO CREATE new column -> fraud_str
# df.fraud = df['fraud'].apply(lambda x: '1' if x=='Y' else '0')


############################################################

"""### Function to make asses model report"""

def asses_model (cls, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test):
    # Asses Train results
    cm_train = confusion_matrix(y_true=y_train,y_pred=cls.predict(X_train),labels=cls.classes_)
    print('Train Confusion Matrix \n'+'#'*10)
    print(pd.DataFrame(cm_train,index=cls.classes_,columns=cls.classes_))
    print('\n Train Classification Report \n'+'#'*10)
    print(classification_report(y_true=y_train,y_pred=cls.predict(X_train)))

    print('\n'+'#'*70 +'\n')

    # Asses Test results
    cm_test = confusion_matrix(y_true=y_test,y_pred=cls.predict(X_test),labels=cls.classes_)
    print('Test Confusion Matrix \n'+'#'*10)
    print(pd.DataFrame(cm_test,index=cls.classes_,columns=cls.classes_))
    print('\n Test Classification Report \n'+'#'*10)
    print(classification_report(y_true=y_test,y_pred=cls.predict(X_test)))

    cls_report=classification_report(y_true=y_test,y_pred=cls.predict(X_test),output_dict=True)
    return cls_report

"""### Preparing Random Under-Samplling (rus) and Random Over-Samplling (ros) for model validation"""

# Under sampling correction train set
rus = RandomUnderSampler(random_state=42)
X_train_rus, y_train_rus= rus.fit_resample(X_train, y_train)
# print('X_train_rus')
# print(X_train_rus.shape)

# Under sampling correction train set
ros = RandomOverSampler(random_state=42)
X_train_ros, y_train_ros= ros.fit_resample(X_train, y_train)
# print('X_train_ros')
print('y train set immbalanced')
print(sorted(Counter(y_train).items()))
print('\nUnder Sampling trainset')
print(sorted(Counter(y_train_rus).items()))
print('\nOver Sampling trainset')
print(sorted(Counter(y_train_ros).items()))

"""###standard scaler"""

# standard scaler pipeline



def get_cols_ss(df):
  return df[['avg_amount_trans_per_day', 'trans_amount', 'total_decline_per_day', 'avg_daily_chargeback_amount', '6_month_avg_chargeback_amount', '6_month_chbk_freq']]
ss_selector = FunctionTransformer(func=get_cols_ss, validate=False)


ss_pipeline = Pipeline([('ss_selector ', ss_selector),
                           ('ss', StandardScaler())])

# TO ADD MORE DATA TRANSFORMATION

"""## Base Line Model- Decision Tree default parameters"""

fraud_dt = DecisionTreeClassifier()#(max_depth=7)
fraud_dt.fit(X_train,y_train)
base_model_report=asses_model (fraud_dt)

print(pd.Series(fraud_dt.feature_importances_,
                index=X.columns).sort_values(ascending=False))

fi=pd.Series(fraud_dt.feature_importances_,
                index=X.columns).sort_values(ascending=False)
fi

# feature importances - plots

md = 4

pds = pd.Series(fraud_dt.feature_importances_,
                index=X.columns).sort_values(ascending=False)
  
importance = pds

names = X_train.columns

model_type = 'Decision Tree'


#Create arrays from feature importance and feature names
feature_importance = np.array(importance)

feature_names = np.array(names)

#Create a DataFrame using a Dictionary
data={'feature_names':feature_names,'feature_importance':feature_importance}
fi_df = pd.DataFrame(data)

#Sort the DataFrame in order decreasing feature importance
fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)

#Define size of bar plot
plt.figure(figsize=(4,6))
#Plot Searborn bar chart
sb.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])
#Add chart labels
plt.title('FEATURE IMPORTANCE | ' + model_type)
plt.xlabel('FEATURE IMPORTANCE')
plt.ylabel('FEATURE NAMES')

"""## Random Forest"""

# train
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
rf_report=asses_model(rf)

"""####Random Forest Under Sampling"""

rus_model= rf.fit(X_train_rus, y_train_rus)
rf_rus_report=asses_model(rus_model)

"""####Random Forest Under Sampling"""

ros_model = rf.fit(X_train_ros, y_train_ros)
rf_ros_report=asses_model(ros_model)

"""## Random Forest Model

###Train
"""

trans_pipeline = FeatureUnion([('ss_pipeline ', ss_pipeline)])

# full pipeline

# full_pipeline = FeatureUnion([('trans_pipeline', trans_pipeline),
#                           ('clf_pipeline', clf_pipeline)])

# train - transformations
trans_pipeline.fit(X_train, y_train)

# train - fit
clf = RandomForestClassifier(max_depth=6, n_estimators=6)
clf.fit(X_train, y_train)

# train - predict
y_train_pred = clf.predict(X_train)


# print(f"train accuracy: {clf.score(X_train, y_train):.2f}")

"""### Assesing RF train """

# train - valuation

# train - precision score
print(f"Train precision: {precision_score(y_train, y_train_pred, average='weighted')} \n") 
train_precision = precision_score(y_train, y_train_pred, average='weighted')

# train - confusion matrix
cm_rf = confusion_matrix(y_true=y_train,
                         y_pred=clf.predict(X_train),
                         labels=clf.classes_)

print('Train Confusion Matrix')
print(pd.DataFrame(cm_rf,
                   index=clf.classes_,
                   columns=clf.classes_))

# train - classification report
target_names = ['Normal', 'Fraud']
print('Train Classification Report')
print(classification_report(y_true=y_train,
                            y_pred=clf.predict(X_train),
                            target_names=target_names))

# # train - grid search

# # grid search score in pipeline
# params = {'max_depth': [10, 3],
#           'n_estimators': [4, 5]}

# gs = GridSearchCV(clf, param_grid=params, cv=5, scoring='recall')
# gs.fit(X_train, y_train)

# GridSearchCV.get_params().keys()

# Get and reshape confusion matrix data
matrix = confusion_matrix(y_train, y_train_pred)
matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]



# Build the plot
plt.figure(figsize=(16,7))
sb.set(font_scale=1.4)
sb.heatmap(matrix, annot=True, annot_kws={'size':10},
            cmap=plt.cm.Greens, linewidths=0.2)

# Add labels to the plot
target_names = ['Normal', 'Fraud']

class_names = target_names
tick_marks = np.arange(len(class_names))
tick_marks2 = tick_marks + 0.5
plt.xticks(tick_marks, class_names, rotation=0)
plt.yticks(tick_marks2, class_names, rotation=0)
# plt.xlabel('y_train_pred')
# plt.ylabel('y_train')
plt.title('Confusion Matrix for Random Forest Model - Train set')
plt.show()



"""###Test"""

# test - transformations
trans_pipeline.fit(X_test, y_test)

# test - fit
clf = RandomForestClassifier(max_depth=6, n_estimators=6)
clf.fit(X_test, y_test)

# test - predict
y_test_pred = clf.predict(X_test)

# test - valuation

# test - precision score
print(f"Test precision: {precision_score(y_test, y_test_pred, average='weighted')} \n") 


# test - confusion matrix
cm_rf = confusion_matrix(y_true=y_test,
                         y_pred=clf.predict(X_test),
                         labels=clf.classes_)

print('Test Confusion Matrix')
print(pd.DataFrame(cm_rf,
                   index=clf.classes_,
                   columns=clf.classes_))

# test - classification report
target_names = ['Normal', 'Fraud']
print('Test Classification Report')
print(classification_report(y_true=y_test,
                            y_pred=clf.predict(X_test),
                            target_names=target_names))



"""###Best params"""

# grid search


my_param_grid = [{'max_depth': [3, 4, 5],
                  # 'random_state': [0],
                  'n_estimators': [5, 15, 30, 50, 100, 200]}]



for metric in ['precision_weighted', 'recall_weighted']:
  for k in [3, 4, 5, 6]:
    start = time.time()
    clf_gs = GridSearchCV(clf, my_param_grid, cv=k, scoring=metric)
    clf_gs.fit(X_train, y_train)

    print(f"\n---- metric = {metric}, folds = {k} ----- \n")
    print("end = " , time.time() - start)
    print("train = ", clf_gs.score(X_train, y_train))
    print("test = ", clf_gs.score(X_test, y_test))
    print("best model = \n", clf_gs.best_estimator_.get_params())
    print("time = ", time.time() - start)

# best params - transformations
trans_pipeline.fit(X_test, y_test)

# best params - fit
clf_bp = RandomForestClassifier(max_depth=4, n_estimators=50)
clf_bp.fit(X_test, y_test)

# tebest paramsst - predict
y_test_pred_bp = clf_bp.predict(X_test)

# best params - valuation

# best params - precision score
print(f"best params precision: {precision_score(y_test, y_test_pred_bp, average='weighted')} \n") 


# best params - confusion matrix
cm_clf_bp = confusion_matrix(y_true=y_test,
                         y_pred=clf_bp.predict(X_test),
                         labels=clf_bp.classes_)

print('best params Confusion Matrix')
print(pd.DataFrame(cm_clf_bp,
                   index=clf_bp.classes_,
                   columns=clf_bp.classes_))

# best params - classification report
target_names = ['Normal', 'Fraud']
print('best params Classification Report')
print(classification_report(y_true=y_test,
                            y_pred=clf_bp.predict(X_test),
                            target_names=target_names))

# feature importances - plots

md = 4

pds = pd.Series(clf.feature_importances_,
                index=X_train.columns).sort_values(ascending=False)
  
importance = pds

names = X_train.columns

model_type = 'RANDOM FOREST'


#Create arrays from feature importance and feature names
feature_importance = np.array(importance)

feature_names = np.array(names)

#Create a DataFrame using a Dictionary
data={'feature_names':feature_names,'feature_importance':feature_importance}
fi_df = pd.DataFrame(data)

#Sort the DataFrame in order decreasing feature importance
fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)

#Define size of bar plot
plt.figure(figsize=(10,8))
#Plot Searborn bar chart
sb.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])
#Add chart labels
plt.title('FEATURE IMPORTANCE | ' + model_type)
plt.xlabel('FEATURE IMPORTANCE')
plt.ylabel('FEATURE NAMES')

# # feature importances by Max Depth - plots

# fi_md_df = pd.DataFrame()

# for md in [1, 5, 10, 20, None]:

#   clf_md = RandomForestClassifier(max_depth=md, n_estimators=6)
#   clf_md.fit(X_train, y_train)

#   pds = pd.Series(clf_md.feature_importances_,
#                   index=X_train.columns).sort_values(ascending=False)
#   # fi_md_df[md] = [f"{i}___{v}" for i,v in zip(pds.index, pds)]
  
#   plot_feature_importance(pds, X_train.columns,f"RANDOM FOREST | Max Depth = {md} ")

clf.get_params()

# cross validation
folds = [2, 3, 4, 5, 10, 15]

results = {k:cross_val_score(RandomForestClassifier(), X_train, y_train, cv=k, scoring='accuracy') for k in folds}

results

# cross validation

import seaborn as sns


variance = []
for k,v in results.items():
  sns.distplot(v, kde=True, bins=20, hist=True)
  var = np.var(v)
  variance.append(var)

  plt.title(f"k ={k}, average ={np.mean(v):.2}, variance = {var:.4}")
  plt.show()

ax = sns.regplot(folds, variance)
plt.title(f"variance vs folds")
plt.show()

# cross validation
k = 8
scores = cross_val_score(RandomForestClassifier(), X_train, y_train, cv=k, scoring='accuracy')

print("Score : " + (k * "{:.2f} ").format(*scores))

"""## Logistic Regression Model"""

fraud_lr = LogisticRegression()
fraud_lr.fit(X_train,y_train)
lr_report=asses_model(fraud_lr)

"""#### LR Under Sampling"""

rus_lr_model= fraud_lr.fit(X_train_rus, y_train_rus)
lr_rus_report=asses_model(rus_lr_model)

"""#### LR Over Sampling"""

rus_lr_model= fraud_lr.fit(X_train_rus, y_train_rus)
lr_rus_report=asses_model(rus_lr_model)

my_param_grid = {'max_iter': [300,400,500],'class_weight':['balanced',{0:1,1:1.2},{0:1,1:1.5},{0:1,1:2},{0:1,1:2.5}]}
LR_gs = GridSearchCV(LogisticRegression(), my_param_grid, cv=5, scoring='accuracy')
LR_gs.fit(X_train, y_train)
print("Best parameters:", LR_gs.best_params_)

"""#### LR with best params"""

fraud_lr_best_param = LogisticRegression(class_weight= {0:1,1:1.5},max_iter=300)
fraud_lr_best_param.fit(X_train,y_train)
fraud_lr_best_param_report=asses_model(fraud_lr_best_param)

fraud_lr_best_param_report

lr_scaled=make_pipeline(StandardScaler(),LogisticRegression(max_iter=300))
lr_scaled.fit(X_train,y_train)
lr_scaled_report=asses_model(lr_scaled)

"""## XGboost"""

fraud_XG = XGBClassifier()
fraud_XG.fit(X_train,y_train)
fraud_XG_report=asses_model(fraud_XG)

"""### XGboost Grid search"""

my_param_grid = {'learning_rate': [0.01,0.1,0.5,1],'max_depth':[1,2,4,8,10], 'n_estimators':[10,50,100,200,300,500]}
clf_gs = GridSearchCV(fraud_XG, my_param_grid, cv=5, scoring='accuracy')
clf_gs.fit(X_train, y_train)
print("Best parameters:", clf_gs.best_params_)

"""### XGboost with best params"""

fraud_XG_BP = XGBClassifier(learning_rate = 0.1, max_depth = 3, n_estimators = 500)
fraud_XG_BP.fit(X_train,y_train)
fraud_XG_BP_report=asses_model(fraud_XG_BP)

"""#### XGboost Under Sampling"""

rus_XG_model= fraud_XG_BP.fit(X_train_rus, y_train_rus)
XG_rus_report=asses_model(rus_XG_model)

"""####XGboost Over Sampling"""

ros_XG_model= fraud_XG_BP.fit(X_train_ros, y_train_ros)
XG_ros_report=asses_model(ros_XG_model)

"""# Summery"""

# v1.1
# baseline Decition Tree f1 

base_model_dict = {'Normal': base_model_report['0']['f1-score'],
        'Fraud': base_model_report['1']['f1-score']}

################################################################################
# Random Forest

rf_report_dict = {'Normal': rf_report['0']['f1-score'],
        'Fraud': rf_report['1']['f1-score']}

rf_rus_report_dict = {'Normal': rf_rus_report['0']['f1-score'],
        'Fraud': rf_rus_report['1']['f1-score']}
rf_ros_report_dict = {'Normal': rf_ros_report['0']['f1-score'],
        'Fraud': rf_ros_report['1']['f1-score']}

################################################################################
# Logistic Regression 

lr_report_dict = {'Normal': lr_report['0']['f1-score'],
        'Fraud': lr_report['1']['f1-score']}

lr_rus_report_dict = {'Normal': lr_rus_report['0']['f1-score'],
        'Fraud': lr_rus_report['1']['f1-score']}

lr_ros_report_dict = {'Normal': lr_ros_report['0']['f1-score'],
        'Fraud': lr_ros_report['1']['f1-score']}

fraud_lr_best_param_report_dict = {'Normal': fraud_lr_best_param_report['0']['f1-score'],
        'Fraud': fraud_lr_best_param_report['1']['f1-score']}
        
  
################################################################################
# #  XGboost 

fraud_XG_report_dict = {'Normal': fraud_XG_report['0']['f1-score'],
        'Fraud': fraud_XG_report['1']['f1-score']}

fraud_XG_BP_report_dict = {'Normal': fraud_XG_BP_report['0']['f1-score'],
        'Fraud': fraud_XG_BP_report['1']['f1-score']}

XG_rus_report_dict = {'Normal': XG_rus_report['0']['f1-score'],
        'Fraud': XG_rus_report['1']['f1-score']}

XG_ros_report_dict = {'Normal': XG_ros_report['0']['f1-score'],
        'Fraud': XG_ros_report['1']['f1-score']}


################################################################################


data_dict = {
    'Baseline Model- Decision Tree': base_model_dict,
    'Random Forest Best Param Model':rf_report_dict,
    'RF Under-Sampling': rf_rus_report_dict,
    'RF Over-Sampling': rf_ros_report_dict,
    'Logistic regression Model':lr_report_dict,
    'LR Under-Sampling': lr_rus_report_dict,
    'LR Over-Sampling': lr_ros_report_dict,
    'LR Best Params Model': fraud_lr_best_param_report_dict,
    'XGboost default Model':fraud_XG_report_dict,
    'XGboost Best Params': fraud_XG_BP_report_dict,
    'XGboost Under-Sampling': XG_rus_report_dict,
    'XGboost Over-Sampling': XG_ros_report_dict,

}

summery = pd.DataFrame.from_dict(data_dict, orient='index', dtype=None, columns=None)
summery.sort_values(by=['Fraud'], ascending=False)

value_list = list(sort_summery)
value_list

#Configure your x and y values from the dictionary:
x= list(data_dict.keys())
y=list(data_dict.values())
#Create the graph
fig = plt.figure(figsize=(20,7))
ax = fig.gca()
sb.barplot(x=data.index,y=data['Fraud'],ax=ax)
ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha="right")
plt.tight_layout()
plt.show()

"""# Validation on best model"""

fraud_lr_best_param_report=asses_model(fraud_lr_best_param, X_test=X_valid, y_test=y_valid)